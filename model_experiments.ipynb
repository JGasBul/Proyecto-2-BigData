{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b89a7f1-84d6-420a-8e48-e7176bb4e238",
   "metadata": {},
   "source": [
    "# Big Data - Proyecto MLLib\n",
    "# Experimentación con Modelos de Machine Learning\n",
    "\n",
    "# Inicializar Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1fe377-5ca1-4af9-8eaa-b1e024640a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import time\n",
    "import json\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Hotel Booking Cancellation - Model Experiments\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7d1f8c-7ff9-49df-8826-1ca3500b9f39",
   "metadata": {},
   "source": [
    "# Cargar datos preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f20986-236b-41a9-8200-ac1fb1a506fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CARGANDO DATOS PREPROCESADOS ===\")\n",
    "\n",
    "train_df = spark.read.parquet(\"train_processed\")\n",
    "val_df = spark.read.parquet(\"val_processed\")\n",
    "\n",
    "print(f\"Datos de entrenamiento: {train_df.count()} registros\")\n",
    "print(f\"Datos de validación: {val_df.count()} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8160ed84-af91-4770-805e-d67c91ab5cfd",
   "metadata": {},
   "source": [
    "# Verificar esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b4129e-5eb3-45a8-a546-973aea6f4ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEsquema de los datos:\")\n",
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25413fc4-db1f-477c-9c69-f666f0aa40ec",
   "metadata": {},
   "source": [
    "# Configurar evaluadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e16ebf-d771-46f9-9770-6a9cca5fdc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_evaluator = BinaryClassificationEvaluator(\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    labelCol=\"is_canceled\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "multiclass_evaluator = MulticlassClassificationEvaluator(\n",
    "    predictionCol=\"prediction\",\n",
    "    labelCol=\"is_canceled\",\n",
    "    metricName=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce5dd4d-c854-48dc-8199-f3b7258fa507",
   "metadata": {},
   "source": [
    "# Función para evaluar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f66fd2e-83ad-4848-a506-c9f8a0681ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_data, val_data, model_name):\n",
    "    \"\"\"\n",
    "    Evalúa un modelo y retorna métricas completas\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    fitted_model = model.fit(train_data)\n",
    "    \n",
    "    # Predicciones en validación\n",
    "    predictions = fitted_model.transform(val_data)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    auc = binary_evaluator.evaluate(predictions)\n",
    "    f1 = multiclass_evaluator.evaluate(predictions)\n",
    "    \n",
    "    # Métricas adicionales\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "        predictionCol=\"prediction\", labelCol=\"is_canceled\", metricName=\"accuracy\"\n",
    "    )\n",
    "    accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "    \n",
    "    precision_evaluator = MulticlassClassificationEvaluator(\n",
    "        predictionCol=\"prediction\", labelCol=\"is_canceled\", metricName=\"weightedPrecision\"\n",
    "    )\n",
    "    precision = precision_evaluator.evaluate(predictions)\n",
    "    \n",
    "    recall_evaluator = MulticlassClassificationEvaluator(\n",
    "        predictionCol=\"prediction\", labelCol=\"is_canceled\", metricName=\"weightedRecall\"\n",
    "    )\n",
    "    recall = recall_evaluator.evaluate(predictions)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    results = {\n",
    "        \"model\": model_name,\n",
    "        \"auc\": auc,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"training_time\": training_time\n",
    "    }\n",
    "    \n",
    "    return results, fitted_model, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15405f1a-8bbe-468e-842c-5ad7279c7fe7",
   "metadata": {},
   "source": [
    "# FASE 1: EVALUACIÓN DE MODELOS BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3705b587-4af1-43d1-bd9e-2d99cc1a5e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== FASE 1: EVALUACIÓN DE MODELOS BASE ===\")\n",
    "\n",
    "# Lista para almacenar todos los resultados\n",
    "all_results = []\n",
    "fitted_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d792119e-c7a0-4e8d-a637-0692de9057c9",
   "metadata": {},
   "source": [
    "# 1. LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefdc045-2ca2-4636-870f-98f552369d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- EXPERIMENTO 1: LOGISTIC REGRESSION ---\")\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"is_canceled\",\n",
    "    maxIter=100\n",
    ")\n",
    "\n",
    "lr_results, lr_model, lr_predictions = evaluate_model(lr, train_df, val_df, \"Logistic Regression\")\n",
    "all_results.append(lr_results)\n",
    "fitted_models[\"Logistic Regression\"] = lr_model\n",
    "\n",
    "print(f\"AUC: {lr_results['auc']:.4f}\")\n",
    "print(f\"F1-Score: {lr_results['f1']:.4f}\")\n",
    "print(f\"Accuracy: {lr_results['accuracy']:.4f}\")\n",
    "print(f\"Tiempo: {lr_results['training_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ce562-fc9e-45dc-a49b-c89c84c41adf",
   "metadata": {},
   "source": [
    "# 2.RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad238efa-9bf0-4928-aeef-7e9cd4cb306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- EXPERIMENTO 2: RANDOM FOREST ---\")\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"is_canceled\",\n",
    "    numTrees=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rf_results, rf_model, rf_predictions = evaluate_model(rf, train_df, val_df, \"Random Forest\")\n",
    "all_results.append(rf_results)\n",
    "fitted_models[\"Random Forest\"] = rf_model\n",
    "\n",
    "print(f\"AUC: {rf_results['auc']:.4f}\")\n",
    "print(f\"F1-Score: {rf_results['f1']:.4f}\")\n",
    "print(f\"Accuracy: {rf_results['accuracy']:.4f}\")\n",
    "print(f\"Tiempo: {rf_results['training_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c9c664-72a4-42c5-b599-ea3ff0ba2451",
   "metadata": {},
   "source": [
    "# 3. GRADIENT BOOSTED TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f4fd7-7054-4cc3-8e17-5be85e41fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- EXPERIMENTO 3: GRADIENT BOOSTED TREES ---\")\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"is_canceled\",\n",
    "    maxIter=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "gbt_results, gbt_model, gbt_predictions = evaluate_model(gbt, train_df, val_df, \"Gradient Boosted Trees\")\n",
    "all_results.append(gbt_results)\n",
    "fitted_models[\"Gradient Boosted Trees\"] = gbt_model\n",
    "\n",
    "print(f\"AUC: {gbt_results['auc']:.4f}\")\n",
    "print(f\"F1-Score: {gbt_results['f1']:.4f}\")\n",
    "print(f\"Accuracy: {gbt_results['accuracy']:.4f}\")\n",
    "print(f\"Tiempo: {gbt_results['training_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791c762c-2291-4ebb-b6f1-6b766fd60028",
   "metadata": {},
   "source": [
    "# 4. DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a2d81c-6d1a-4ca1-a4f0-f6a2fbb206c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- EXPERIMENTO 4: DECISION TREE ---\")\n",
    "dt = DecisionTreeClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"is_canceled\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "dt_results, dt_model, dt_predictions = evaluate_model(dt, train_df, val_df, \"Decision Tree\")\n",
    "all_results.append(dt_results)\n",
    "fitted_models[\"Decision Tree\"] = dt_model\n",
    "\n",
    "print(f\"AUC: {dt_results['auc']:.4f}\")\n",
    "print(f\"F1-Score: {dt_results['f1']:.4f}\")\n",
    "print(f\"Accuracy: {dt_results['accuracy']:.4f}\")\n",
    "print(f\"Tiempo: {dt_results['training_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7801e6a1-3c05-402e-94c5-71235ed09351",
   "metadata": {},
   "source": [
    "# 5. NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69b4687-d3d1-4473-9946-d11ef78dc35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- EXPERIMENTO 5: NAIVE BAYES ---\")\n",
    "nb = NaiveBayes(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"is_canceled\"\n",
    ")\n",
    "\n",
    "nb_results, nb_model, nb_predictions = evaluate_model(nb, train_df, val_df, \"Naive Bayes\")\n",
    "all_results.append(nb_results)\n",
    "fitted_models[\"Naive Bayes\"] = nb_model\n",
    "\n",
    "print(f\"AUC: {nb_results['auc']:.4f}\")\n",
    "print(f\"F1-Score: {nb_results['f1']:.4f}\")\n",
    "print(f\"Accuracy: {nb_results['accuracy']:.4f}\")\n",
    "print(f\"Tiempo: {nb_results['training_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b79bc-e430-4125-bd92-730ce6ebd0d9",
   "metadata": {},
   "source": [
    "# RANKING DE MODELOS BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e25d660-1f55-4d7a-bda8-37a00f97cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== RANKING DE MODELOS BASE ===\")\n",
    "\n",
    "# Ordenar por F1-Score\n",
    "sorted_results = sorted(all_results, key=lambda x: x['f1'], reverse=True)\n",
    "\n",
    "print(\"\\nRanking por F1-Score:\")\n",
    "for i, result in enumerate(sorted_results, 1):\n",
    "    print(f\"{i}. {result['model']}: F1={result['f1']:.4f}, AUC={result['auc']:.4f}\")\n",
    "\n",
    "# Seleccionar automáticamente los top 3\n",
    "top_3_models = sorted_results[:3]\n",
    "print(f\"\\n=== SELECCIONADOS PARA OPTIMIZACIÓN (TOP 3) ===\")\n",
    "for i, result in enumerate(top_3_models, 1):\n",
    "    print(f\"{i}. {result['model']}: F1={result['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30002d83-206f-4cef-8005-6fe35b6d4112",
   "metadata": {},
   "source": [
    "# FASE 2: OPTIMIZACIÓN CON GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3fac54-69f5-4072-aecb-2396845e02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== FASE 2: OPTIMIZACIÓN CON GRID SEARCH ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609a55fb-b6ff-487e-8cc8-708116d3c97e",
   "metadata": {},
   "source": [
    "# Metodo para Crear un grid de parámetros específico para cada tipo de modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b3eef7-15dd-4ef3-9788-6b216faa7d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_param_grid(model_name, model_obj):\n",
    "    \"\"\"\n",
    "    Crea grid de parámetros específico para cada tipo de modelo\n",
    "    \"\"\"\n",
    "    if \"Logistic Regression\" in model_name:\n",
    "        return ParamGridBuilder() \\\n",
    "            .addGrid(model_obj.regParam, [0.01, 0.1, 1.0]) \\\n",
    "            .addGrid(model_obj.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "            .addGrid(model_obj.maxIter, [100, 200]) \\\n",
    "            .build()\n",
    "    \n",
    "    elif \"Random Forest\" in model_name:\n",
    "        return ParamGridBuilder() \\\n",
    "            .addGrid(model_obj.numTrees, [50, 100, 200]) \\\n",
    "            .addGrid(model_obj.maxDepth, [5, 10, 15]) \\\n",
    "            .addGrid(model_obj.minInstancesPerNode, [1, 5, 10]) \\\n",
    "            .build()\n",
    "    \n",
    "    elif \"Gradient Boosted Trees\" in model_name:\n",
    "        return ParamGridBuilder() \\\n",
    "            .addGrid(model_obj.maxIter, [50, 100, 150]) \\\n",
    "            .addGrid(model_obj.maxDepth, [3, 5, 7]) \\\n",
    "            .addGrid(model_obj.stepSize, [0.01, 0.1, 0.2]) \\\n",
    "            .build()\n",
    "    \n",
    "    elif \"Decision Tree\" in model_name:\n",
    "        return ParamGridBuilder() \\\n",
    "            .addGrid(model_obj.maxDepth, [5, 10, 15, 20]) \\\n",
    "            .addGrid(model_obj.minInstancesPerNode, [1, 5, 10, 20]) \\\n",
    "            .addGrid(model_obj.impurity, [\"gini\", \"entropy\"]) \\\n",
    "            .build()\n",
    "    \n",
    "    elif \"Naive Bayes\" in model_name:\n",
    "        return ParamGridBuilder() \\\n",
    "            .addGrid(model_obj.smoothing, [0.1, 1.0, 2.0, 5.0]) \\\n",
    "            .build()\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Tipo de modelo no reconocido: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676fbbff-769d-4514-bcff-2fac6857365a",
   "metadata": {},
   "source": [
    "# Metodo para optimizar un modelo específico usando Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ff284a-ca74-49eb-805c-5845e7a52633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(model_name, original_result):\n",
    "    \"\"\"\n",
    "    Optimiza un modelo específico usando Grid Search\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- OPTIMIZANDO: {model_name.upper()} ---\")\n",
    "    \n",
    "    # Crear nuevo modelo del mismo tipo\n",
    "    if \"Logistic Regression\" in model_name:\n",
    "        model_obj = LogisticRegression(featuresCol=\"features\", labelCol=\"is_canceled\", maxIter=100)\n",
    "    elif \"Random Forest\" in model_name:\n",
    "        model_obj = RandomForestClassifier(featuresCol=\"features\", labelCol=\"is_canceled\", numTrees=100, seed=42)\n",
    "    elif \"Gradient Boosted Trees\" in model_name:\n",
    "        model_obj = GBTClassifier(featuresCol=\"features\", labelCol=\"is_canceled\", maxIter=100, seed=42)\n",
    "    elif \"Decision Tree\" in model_name:\n",
    "        model_obj = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"is_canceled\", seed=42)\n",
    "    elif \"Naive Bayes\" in model_name:\n",
    "        model_obj = NaiveBayes(featuresCol=\"features\", labelCol=\"is_canceled\")\n",
    "    \n",
    "    # Crear grid de parámetros\n",
    "    param_grid = create_param_grid(model_name, model_obj)\n",
    "    \n",
    "    # Configurar Cross Validator\n",
    "    cv = CrossValidator(\n",
    "        estimator=model_obj,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=multiclass_evaluator,  # F1-score\n",
    "        numFolds=3,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Ejecutando Grid Search con {len(param_grid)} combinaciones...\")\n",
    "    \n",
    "    # Entrenar\n",
    "    start_time = time.time()\n",
    "    cv_model = cv.fit(train_df)\n",
    "    optimization_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluar mejor modelo\n",
    "    best_predictions = cv_model.transform(val_df)\n",
    "    best_auc = binary_evaluator.evaluate(best_predictions)\n",
    "    best_f1 = multiclass_evaluator.evaluate(best_predictions)\n",
    "    \n",
    "    # Calcular métricas adicionales\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "        predictionCol=\"prediction\", labelCol=\"is_canceled\", metricName=\"accuracy\"\n",
    "    )\n",
    "    best_accuracy = accuracy_evaluator.evaluate(best_predictions)\n",
    "    \n",
    "    precision_evaluator = MulticlassClassificationEvaluator(\n",
    "        predictionCol=\"prediction\", labelCol=\"is_canceled\", metricName=\"weightedPrecision\"\n",
    "    )\n",
    "    best_precision = precision_evaluator.evaluate(best_predictions)\n",
    "    \n",
    "    recall_evaluator = MulticlassClassificationEvaluator(\n",
    "        predictionCol=\"prediction\", labelCol=\"is_canceled\", metricName=\"weightedRecall\"\n",
    "    )\n",
    "    best_recall = recall_evaluator.evaluate(best_predictions)\n",
    "    \n",
    "    # Calcular mejoras\n",
    "    original_f1 = original_result['f1']\n",
    "    f1_improvement = best_f1 - original_f1\n",
    "    improvement_percent = (f1_improvement / original_f1) * 100\n",
    "    \n",
    "    print(f\"F1-Score original: {original_f1:.4f}\")\n",
    "    print(f\"F1-Score optimizado: {best_f1:.4f}\")\n",
    "    print(f\"Mejora: {f1_improvement:+.4f} ({improvement_percent:+.1f}%)\")\n",
    "    print(f\"AUC: {best_auc:.4f}\")\n",
    "    print(f\"Tiempo optimización: {optimization_time:.1f}s\")\n",
    "    \n",
    "    # Mostrar mejores parámetros\n",
    "    best_params = cv_model.bestModel.extractParamMap()\n",
    "    print(\"Mejores parámetros:\")\n",
    "    for param, value in best_params.items():\n",
    "        if hasattr(param, 'name'):\n",
    "            print(f\"  {param.name}: {value}\")\n",
    "    \n",
    "    # Crear resultado\n",
    "    optimized_result = {\n",
    "        \"model\": f\"{model_name} (Grid Search)\",\n",
    "        \"auc\": best_auc,\n",
    "        \"f1\": best_f1,\n",
    "        \"accuracy\": best_accuracy,\n",
    "        \"precision\": best_precision,\n",
    "        \"recall\": best_recall,\n",
    "        \"training_time\": optimization_time,\n",
    "        \"f1_improvement\": f1_improvement,\n",
    "        \"improvement_percent\": improvement_percent,\n",
    "        \"best_params\": {param.name: value for param, value in best_params.items() if hasattr(param, 'name')}\n",
    "    }\n",
    "    \n",
    "    return optimized_result, cv_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc00992-ba10-4544-b361-7934076d425a",
   "metadata": {},
   "source": [
    "# Optimizar los top 3 modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76796571-1aff-40c4-856a-153d8644beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_results = []\n",
    "optimized_models = {}\n",
    "\n",
    "for result in top_3_models:\n",
    "    model_name = result['model']\n",
    "    try:\n",
    "        opt_result, opt_model = optimize_model(model_name, result)\n",
    "        optimized_results.append(opt_result)\n",
    "        optimized_models[model_name] = opt_model\n",
    "        all_results.append(opt_result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error optimizando {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe5cc60-c4e2-4c1e-ba58-374b0649bb2c",
   "metadata": {},
   "source": [
    "# COMPARACIÓN FINAL: TODOS LOS MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da197eb-cdb6-4830-84c4-8df6d2cea0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== COMPARACIÓN FINAL: TODOS LOS MODELOS ===\")\n",
    "\n",
    "# Tabla comparativa\n",
    "print(\"\\n{:<35} {:<8} {:<8} {:<8} {:<10} {:<12}\".format(\n",
    "    \"Modelo\", \"F1-Score\", \"AUC\", \"Accuracy\", \"Mejora F1\", \"Tiempo(s)\"))\n",
    "print(\"-\" * 85)\n",
    "\n",
    "# Modelos base\n",
    "for result in all_results:\n",
    "    if \"Grid Search\" not in result[\"model\"]:\n",
    "        print(\"{:<35} {:<8.4f} {:<8.4f} {:<8.4f} {:<10} {:<12.2f}\".format(\n",
    "            result['model'], result['f1'], result['auc'], \n",
    "            result['accuracy'], \"-\", result['training_time']))\n",
    "\n",
    "print(\"-\" * 85)\n",
    "\n",
    "# Modelos optimizados\n",
    "for result in all_results:\n",
    "    if \"Grid Search\" in result[\"model\"]:\n",
    "        improvement = f\"+{result['f1_improvement']:.4f}\" if 'f1_improvement' in result else \"N/A\"\n",
    "        print(\"{:<35} {:<8.4f} {:<8.4f} {:<8.4f} {:<10} {:<12.2f}\".format(\n",
    "            result['model'], result['f1'], result['auc'], \n",
    "            result['accuracy'], improvement, result['training_time']))\n",
    "\n",
    "# Encontrar el mejor modelo absoluto\n",
    "best_model_overall = max(all_results, key=lambda x: x['f1'])\n",
    "\n",
    "print(f\"\\n=== MODELO GANADOR ===\")\n",
    "print(f\"🏆 {best_model_overall['model']}\")\n",
    "print(f\"📊 F1-Score: {best_model_overall['f1']:.4f}\")\n",
    "print(f\"📊 AUC: {best_model_overall['auc']:.4f}\")\n",
    "print(f\"📊 Accuracy: {best_model_overall['accuracy']:.4f}\")\n",
    "\n",
    "if 'f1_improvement' in best_model_overall:\n",
    "    print(f\"📈 Mejora vs base: +{best_model_overall['f1_improvement']:.4f} ({best_model_overall['improvement_percent']:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f4b31e-0fbe-4c0d-a59c-5a165c83ced9",
   "metadata": {},
   "source": [
    "# GUARDAR MEJOR MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3862c11b-5f27-453a-9eab-5257e2dd474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== GUARDAR MEJOR MODELO ===\")\n",
    "\n",
    "# Determinar qué modelo guardar\n",
    "best_model_name = best_model_overall['model']\n",
    "\n",
    "if \"Grid Search\" in best_model_name:\n",
    "    # Es un modelo optimizado\n",
    "    base_name = best_model_name.replace(\" (Grid Search)\", \"\")\n",
    "    if base_name in optimized_models:\n",
    "        best_fitted_model = optimized_models[base_name].bestModel\n",
    "        print(f\"Guardando modelo optimizado: {best_model_name}\")\n",
    "    else:\n",
    "        print(f\"Error: No se encontró el modelo optimizado para {base_name}\")\n",
    "        best_fitted_model = None\n",
    "else:\n",
    "    # Es un modelo base\n",
    "    if best_model_name in fitted_models:\n",
    "        best_fitted_model = fitted_models[best_model_name]\n",
    "        print(f\"Guardando modelo base: {best_model_name}\")\n",
    "    else:\n",
    "        print(f\"Error: No se encontró el modelo base para {best_model_name}\")\n",
    "        best_fitted_model = None\n",
    "\n",
    "if best_fitted_model:\n",
    "    best_fitted_model.write().overwrite().save(\"best_model\")\n",
    "    print(\"✅ Mejor modelo guardado en 'best_model/'\")\n",
    "else:\n",
    "    print(\"❌ Error: No se pudo guardar el modelo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54754738-ed94-4606-80c9-03af07b78e07",
   "metadata": {},
   "source": [
    "# ANÁLISIS DE IMPORTANCIA DE CARACTERÍSTICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5cdd45-3a78-478d-a7ee-d793b46e3a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== ANÁLISIS DE IMPORTANCIA DE CARACTERÍSTICAS ===\")\n",
    "\n",
    "# Analizar importancia si el modelo lo soporta\n",
    "if hasattr(best_fitted_model, 'featureImportances') and best_fitted_model.featureImportances is not None:\n",
    "    feature_importances = best_fitted_model.featureImportances.toArray()\n",
    "    \n",
    "    print(f\"\\nTop 10 características más importantes ({best_model_name}):\")\n",
    "    importance_pairs = [(i, importance) for i, importance in enumerate(feature_importances)]\n",
    "    importance_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for i, (feature_idx, importance) in enumerate(importance_pairs[:10]):\n",
    "        print(f\"{i+1:2d}. Característica {feature_idx:3d}: {importance:.4f}\")\n",
    "else:\n",
    "    print(f\"El modelo {best_model_name} no soporta análisis de importancia de características\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c9496b-bfa6-4457-aa16-96657f041f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2913f80b-a8bd-4e73-8675-ce6374f6fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== MATRIZ DE CONFUSIÓN DEL MEJOR MODELO ===\")\n",
    "\n",
    "# Obtener predicciones del mejor modelo\n",
    "if \"Grid Search\" in best_model_name:\n",
    "    base_name = best_model_name.replace(\" (Grid Search)\", \"\")\n",
    "    if base_name in optimized_models:\n",
    "        best_predictions = optimized_models[base_name].transform(val_df)\n",
    "    else:\n",
    "        print(\"Error obteniendo predicciones del modelo optimizado\")\n",
    "        best_predictions = None\n",
    "else:\n",
    "    if best_model_name in fitted_models:\n",
    "        best_predictions = fitted_models[best_model_name].transform(val_df)\n",
    "    else:\n",
    "        print(\"Error obteniendo predicciones del modelo base\")\n",
    "        best_predictions = None\n",
    "\n",
    "if best_predictions:\n",
    "    print(\"\\nMatriz de Confusión:\")\n",
    "    confusion_matrix = best_predictions.groupBy(\"is_canceled\", \"prediction\").count().orderBy(\"is_canceled\", \"prediction\")\n",
    "    confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b9cd8-13e5-43a0-8455-d0d999ca2363",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== ESTADÍSTICAS FINALES ===\")\n",
    "print(f\"✅ Modelos evaluados inicialmente: 5\")\n",
    "print(f\"✅ Modelos optimizados: {len(optimized_results)}\")\n",
    "print(f\"✅ Mejor F1-Score final: {best_model_overall['f1']:.4f}\")\n",
    "print(f\"✅ Tiempo total experimentación: {sum(r['training_time'] for r in all_results):.1f}s\")\n",
    "\n",
    "if optimized_results:\n",
    "    avg_improvement = sum(r['f1_improvement'] for r in optimized_results) / len(optimized_results)\n",
    "    print(f\"✅ Mejora promedio por optimización: +{avg_improvement:.4f}\")\n",
    "\n",
    "# Guardar resultados en JSON\n",
    "final_results = {\n",
    "    \"methodology\": {\n",
    "        \"total_models_evaluated\": 5,\n",
    "        \"models_optimized\": len(optimized_results),\n",
    "        \"selection_criteria\": \"Top 3 by F1-Score\",\n",
    "        \"optimization_method\": \"Grid Search with 3-fold CV\"\n",
    "    },\n",
    "    \"all_results\": all_results,\n",
    "    \"best_model\": best_model_overall,\n",
    "    \"experiment_timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "with open(\"experiment_results.json\", \"w\") as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(\"\\n📁 Todos los resultados guardados en 'experiment_results.json'\")\n",
    "\n",
    "print(\"\\n=== RESUMEN EJECUTIVO ===\")\n",
    "print(\"🎯 PROCESO COMPLETADO:\")\n",
    "print(\"   1. ✅ Evaluación objetiva de 5 algoritmos\")\n",
    "print(\"   2. ✅ Selección automática de top 3 por F1-Score\")\n",
    "print(\"   3. ✅ Optimización con Grid Search + CV\")\n",
    "print(\"   4. ✅ Selección final del mejor modelo global\")\n",
    "print(f\"🏆 RESULTADO: {best_model_overall['model']} - F1: {best_model_overall['f1']:.4f}\")\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
