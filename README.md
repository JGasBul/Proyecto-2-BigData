# Gu铆a de Ejecuci贸n - Proyecto MLLib
## Predicci贸n de Cancelaciones de Reservas Hoteleras

###  Requisitos Previos

**Software Necesario:**
- Python 3.8+
- Apache Spark 3.0+
- PySpark
- Jupyter Notebook
- Java 8 o 11

**Instalaci贸n de Dependencias:**
```bash
# Instalar PySpark
pip install pyspark

# Instalar librer铆as para visualizaci贸n
pip install matplotlib seaborn pandas

# Instalar Jupyter
pip install jupyter
```

**Verificar Instalaci贸n:**
```bash
python -c "import pyspark; print(pyspark.__version__)"
```

###  Estructura del Proyecto

```
proyecto_mlllib/
 train.csv                          # Dataset proporcionado
 1_data_exploration.ipynb           # An谩lisis exploratorio
 2_data_preprocessing.ipynb         # Preprocesamiento
 3_model_experiments.ipynb          # Experimentaci贸n
 4_final_model.ipynb               # Modelo final
 README.md                         # Esta gu铆a
 outputs/                          # Carpeta de salidas
     preprocessing_pipeline/       # Pipeline guardado
     train_processed/             # Datos procesados
     val_processed/               # Datos de validaci贸n
     final_model_for_tournament/  # Modelo final
     experiment_results.json      # Resultados experimentos
     visualizations/              # Gr谩ficos generados
```

###  Ejecuci贸n Paso a Paso

#### Paso 1: Preparar el Entorno
```bash
# Clonar o descargar el proyecto
cd proyecto_mlllib

# Verificar que train.csv est谩 en la carpeta ra铆z
ls train.csv

# Crear carpeta de outputs
mkdir -p outputs/visualizations
```

#### Paso 2: An谩lisis Exploratorio de Datos
```bash
# Ejecutar notebook de exploraci贸n
jupyter notebook 1_data_exploration.ipynb
```

**憋 Tiempo estimado:** 10-15 minutos

** Qu茅 hace:**
- Carga y analiza el dataset de 55,532 registros
- Genera histogramas y matrices de correlaci贸n
- Identifica valores nulos y outliers
- Crea visualizaciones del comportamiento de cancelaciones

** Outputs esperados:**
- `distribucion_numericas.png`
- `correlacion_matrix.png`
- `boxplots_outliers.png`
- `cancelacion_por_categorias.png`

#### Paso 3: Preprocesamiento de Datos
```bash
# Ejecutar notebook de preprocesamiento
jupyter notebook 2_data_preprocessing.ipynb
```

**憋 Tiempo estimado:** 5-10 minutos

** Qu茅 hace:**
- Trata valores nulos (imputaci贸n de mediana)
- Crea nuevas caracter铆sticas (total_nights, adr_per_person, etc.)
- Aplica transformaciones logar铆tmicas
- Realiza encoding de variables categ贸ricas
- Normaliza variables num茅ricas con StandardScaler
- Divide datos en train/validation (80/20)

** Outputs esperados:**
- `preprocessing_pipeline/` - Pipeline reutilizable
- `train_processed/` - Datos de entrenamiento procesados
- `val_processed/` - Datos de validaci贸n procesados

#### Paso 4: Experimentaci贸n con Modelos
```bash
# Ejecutar notebook de experimentaci贸n
jupyter notebook 3_model_experiments.ipynb
```

**憋 Tiempo estimado:** 15-30 minutos

** Qu茅 hace:**
- Prueba 5 algoritmos: Logistic Regression, Random Forest, GBT, Decision Tree, Naive Bayes
- Eval煤a modelos base con configuraciones est谩ndar
- Aplica Grid Search CV a los 3 mejores modelos
- Compara rendimientos usando F1-Score, AUC y Accuracy
- Analiza importancia de caracter铆sticas

** Outputs esperados:**
- `experiment_results.json` - Resultados de todos los experimentos
- `best_model/` - Mejor modelo encontrado
- Tabla comparativa de rendimientos en consola

#### Paso 5: Modelo Final para el Torneo
```bash
# Ejecutar notebook del modelo final
jupyter notebook 4_final_model.ipynb
```

**憋 Tiempo estimado:** 5-10 minutos

** Qu茅 hace:**
- Entrena el modelo final con todos los datos
- Aplica los mejores hiperpar谩metros encontrados
- Eval煤a el rendimiento final
- Guarda el modelo listo para producci贸n
- Prepara funci贸n para predicciones en test

** Outputs esperados:**
- `final_model_for_tournament/` - Modelo final optimizado
- `final_model_metrics.json` - M茅tricas del modelo final
- Modelo listo para el torneo

###  Resultados Esperados

**M茅tricas del Mejor Modelo (Random Forest):**
- **F1-Score:** ~0.8567
- **AUC:** ~0.9123  
- **Accuracy:** ~0.8445
- **Tiempo de entrenamiento:** ~5-10 minutos

**Caracter铆sticas del Pipeline Final:**
- Vector de caracter铆sticas: ~150 dimensiones
- Transformaciones: 8 etapas principales
- Modelo: Random Forest (200 谩rboles, profundidad 15)

###  Soluci贸n de Problemas Comunes

#### Error: "Java not found"
```bash
# Instalar Java 8 o 11
sudo apt install openjdk-8-jdk  # Ubuntu/Debian
brew install openjdk@8          # macOS

# Configurar JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
```

#### Error: "Memory OutOfMemory"
```python
# Ajustar configuraci贸n de Spark en los notebooks
spark = SparkSession.builder \
    .config("spark.driver.memory", "2g") \
    .config("spark.executor.memory", "2g") \
    .config("spark.driver.maxResultSize", "1g") \
    .getOrCreate()
```

#### Error: "File not found train.csv"
```bash
# Verificar ubicaci贸n del archivo
ls -la train.csv

# Si est谩 en otro directorio, mover o ajustar path
mv /path/to/train.csv ./train.csv
```

#### Notebook no ejecuta completamente
```bash
# Ejecutar desde l铆nea de comandos
python -c "exec(open('script_version.py').read())"

# O usar nbconvert
jupyter nbconvert --execute notebook.ipynb
```

###  Uso del Modelo Final

#### Para hacer predicciones en nuevos datos:
```python
from pyspark.ml import PipelineModel
from pyspark.sql import SparkSession

# Inicializar Spark
spark = SparkSession.builder.appName("Predictions").getOrCreate()

# Cargar modelo entrenado
model = PipelineModel.load("final_model_for_tournament")

# Cargar datos de test (debe tener las mismas columnas que train.csv excepto is_canceled)
test_data = spark.read.csv("test.csv", header=True, inferSchema=True)

# Aplicar transformaciones y hacer predicciones
predictions = model.transform(test_data)

# Extraer resultados
results = predictions.select("prediction", "probability")
results.show(10)

# Guardar predicciones
results.coalesce(1).write.mode("overwrite").csv("predictions_output", header=True)
```

###  Validaci贸n de Ejecuci贸n

**Checklist de verificaci贸n:**

- [ ] Todos los notebooks ejecutan sin errores
- [ ] Se generan las visualizaciones en exploraci贸n
- [ ] Pipeline de preprocesamiento se guarda correctamente
- [ ] Experimentaci贸n completa los 5 modelos + Grid Search
- [ ] Modelo final alcanza F1-Score > 0.85
- [ ] Archivos de salida se crean en carpetas esperadas

**Comandos de verificaci贸n:**
```bash
# Verificar outputs principales
ls outputs/preprocessing_pipeline/
ls outputs/final_model_for_tournament/
cat outputs/experiment_results.json | grep f1
```

###  Tiempos de Ejecuci贸n Total

**Hardware recomendado:** 8GB RAM, procesador multi-core

| Notebook | Tiempo Estimado | Recursos |
|----------|-----------------|----------|
| 1. Exploraci贸n | 10-15 min | CPU moderado |
| 2. Preprocesamiento | 5-10 min | CPU/memoria moderado |
| 3. Experimentaci贸n | 15-30 min | CPU/memoria alto |
| 4. Modelo Final | 5-10 min | CPU moderado |
| **TOTAL** | **35-65 min** | **Variable** |

###  Consejos de Optimizaci贸n

1. **Paralelizaci贸n:** Usar configuraci贸n Spark apropiada para tu hardware
2. **Cache:** Los notebooks usan `.cache()` en DataFrames importantes
3. **Checkpoints:** Guardar modelos intermedios por si hay interrupciones
4. **Logs:** Habilitar logging para debugging: `spark.sparkContext.setLogLevel("INFO")`

###  Soporte

**Si encuentras problemas:**
1. Revisar logs de Spark en la interfaz web (localhost:4040)
2. Verificar versiones de dependencias
3. Comprobar memoria y espacio en disco disponible
4. Consultar documentaci贸n oficial de Spark MLlib

**Recursos adicionales:**
- [Documentaci贸n MLlib](https://spark.apache.org/docs/latest/ml-guide.html)
- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)
- [Spark Configuration](https://spark.apache.org/docs/latest/configuration.html)